import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.inspection import PartialDependenceDisplay
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Interface Streamlit
st.set_page_config(page_title="Outil Directive Europ√©enne", page_icon="üìä", layout="wide")

st.markdown("## üîí Acc√®s s√©curis√©")

password = st.text_input("Veuillez entrer le mot de passe :", type="password")

correct_password = "directive2025"

if password != correct_password:
    st.warning("‚õî Mot de passe incorrect ou vide.")
    st.stop()
else:
    st.success("üîì Acc√®s autoris√©.")


st.title("üìä Analyse √©conom√©trique de la r√©mun√©ration")

# üß† Tableau comparatif des mod√®les
st.markdown("### ü§ñ Comparaison des mod√®les disponibles")

comparaison_modeles = pd.DataFrame({
    "Caract√©ristique": [
        "Relation entre variables",
        "Captation des interactions",
        "Sensibilit√© au bruit",
        "Interpr√©tabilit√©",
        "Performance pr√©dictive",
        "Hypoth√®se de lin√©arit√©",
        "Gestion des non-lin√©arit√©s"
    ],
    "R√©gression Lin√©aire": [
        "Lin√©aire",
        "Non",
        "√âlev√©e",
        "üåü Tr√®s bonne",
        "‚ö†Ô∏è Moyenne",
        "Oui",
        "Non"
    ],
    "Arbre de D√©cision": [
        "Segmentaire (si/alors)",
        "Oui (via les branches)",
        "Moyenne",
        "Bonne (arbre lisible)",
        "Bonne",
        "Non",
        "Partielle"
    ],
    "Random Forest": [
        "Complexe (agr√©gation d'arbres)",
        "Oui (automatique)",
        "Faible",
        "‚ùì Moyenne (bo√Æte noire)",
        "üåü Tr√®s bonne",
        "Non",
        "Oui (forte)"
    ]
})

st.dataframe(comparaison_modeles.set_index("Caract√©ristique"))


uploaded_file = st.file_uploader("üìÅ Chargez votre fichier Excel ou CSV", type=["xlsx", "xlsm", "csv"])

if uploaded_file:
    if uploaded_file.name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    else:
        df = pd.read_excel(uploaded_file)

    df = df.copy()
    today = pd.to_datetime("today")
    colonnes_ajoutees = []
    col_replacements = {}

    for col in df.columns:
        col_lower = col.lower()
        if "date" not in col_lower:
            continue

        try:
            dates = pd.to_datetime(df[col], errors='coerce')
            if dates.notna().mean() < 0.5:
                continue
        except:
            continue

        base_col = col.strip()

        # Toujours cr√©er les colonnes d√©riv√©es √† partir de dates explicites
        col_days = f"{base_col} - Anciennet√© (jours)"
        col_year = f"{base_col} - Ann√©e"
        col_month = f"{base_col} - Mois"
        df[col_days] = (today - dates).dt.days
        df[col_year] = dates.dt.year
        df[col_month] = dates.dt.month
        colonnes_ajoutees += [col_days, col_year, col_month]
        col_replacements[col] = col_days

        # Si c‚Äôest une date de naissance, on ajoute l‚Äô√¢ge
        if any(k in base_col.lower() for k in ["birth", "naiss", "dob"]):
            age_col = f"{base_col} - √Çge (ann√©es)"
            df[age_col] = ((today - dates).dt.days / 365.25).round().astype('Int64')
            colonnes_ajoutees.append(age_col)
            col_replacements[col] = age_col

    df.drop(columns=col_replacements.keys(), inplace=True)

    if colonnes_ajoutees:
        st.info("üõ†Ô∏è Colonnes exploitables ajout√©es automatiquement √† partir des dates :\n- " + "\n- ".join(colonnes_ajoutees))

    st.subheader("üìã Aper√ßu des donn√©es")
    st.dataframe(df.head())

    all_columns = df.columns.tolist()
    date_columns = [col for col in all_columns if 'date' in col.lower()]
    non_date_columns = [col for col in all_columns if col not in date_columns]
    target = st.selectbox("üéØ Variable √† expliquer (cible)", non_date_columns)


    # Calculer un R¬≤ pour chaque variable prise seule
    r2_scores = []
    for var in non_date_columns:
        if var == target:
            continue
        try:
            X_single = df[[var]].copy()
            y_single = df[[target]].copy()
            df_temp = pd.concat([X_single, y_single], axis=1).dropna()

            if df_temp.shape[0] < 5:
                continue  # Trop peu d'observations

            X_clean = pd.get_dummies(df_temp[[var]], drop_first=True)
            y_clean = df_temp[target]
            model = LinearRegression()
            model.fit(X_clean, y_clean)
            score = model.score(X_clean, y_clean)
            r2_scores.append((var, score))
        except Exception as e:
            continue


    # Trier par R¬≤ d√©croissant et prendre les top variables
    r2_scores.sort(key=lambda x: x[1], reverse=True)
    top_variables = [x[0] for x in r2_scores[:5]]


    features = st.multiselect(
        "üìå Variables explicatives",
        [col for col in non_date_columns if col != target],
        default=top_variables
    )

    selected_date_features = st.multiselect("üìÜ Variables de type date (transform√©es)", [col for col in date_columns if col != target])

    # On fusionne les deux listes de variables explicatives s√©lectionn√©es
    features = features + selected_date_features

    model_type = st.selectbox("üß† Mod√®le √† appliquer", ["R√©gression Lin√©aire", "Arbre de D√©cision", "Random Forest"])

    if target and features:
        X = df[features].copy()
        y = df[target]

        # Supprimer colonnes datetime
        datetime_cols = X.select_dtypes(include=["datetime", "datetime64[ns]"]).columns
        if len(datetime_cols) > 0:
            st.warning(f"üïí Colonnes datetime ignor√©es : {', '.join(datetime_cols)}")
            X = X.drop(columns=datetime_cols)

        # One-hot encoding pour les colonnes cat√©gorielles
        X = pd.get_dummies(X, drop_first=True)

        # Suppression des lignes avec NaN
        if X.isnull().any().any() or y.isnull().any():
            st.warning("‚ö†Ô∏è Valeurs manquantes d√©tect√©es ‚Äî lignes concern√©es supprim√©es.")
            df_clean = pd.concat([X, y], axis=1).dropna()
            X = df_clean[X.columns]
            y = df_clean[y.name]

        if X.shape[0] == 0:
            st.error("‚ùå Aucune ligne exploitable apr√®s nettoyage (valeurs manquantes). Veuillez v√©rifier les donn√©es charg√©es ou ajuster les variables s√©lectionn√©es.")
            st.stop()
        else:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


        if model_type == "R√©gression Lin√©aire":
            model = LinearRegression()
        elif model_type == "Arbre de D√©cision":
            model = DecisionTreeRegressor(max_depth=4, random_state=42)
        else:
            model = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)

        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)
        score = r2_score(y_test, y_pred)

        st.markdown("---")
        st.markdown(
            "<div style='background-color:#e6f4ff;padding:10px;border-radius:8px;border-left:5px solid #3399ff;'>"
            "<h4>üìà Score du mod√®le</h4>",
            unsafe_allow_html=True
        )

        st.subheader("üìà R√©sultats")
        st.markdown("""
        <small>‚ÑπÔ∏è Le **score R¬≤** indique la proportion de la variance de la variable cible expliqu√©e par le mod√®le :
        - **R¬≤ = 1** : pr√©dictions parfaites,
        - **R¬≤ = 0** : aucune capacit√© explicative,
        - **R¬≤ < 0** : pire qu‚Äôun mod√®le constant.
        </small>
        """, unsafe_allow_html=True)

        if score < -1 or score > 1e6:
            st.error(f"‚ùå Score R¬≤ incoh√©rent : {score:.2e} ‚Äî v√©rifiez vos donn√©es !")
        else:
            st.markdown(f"""
            <div style="background-color:#f0f8ff;padding:10px;border-radius:8px;border:1px solid #d0d0d0;">
            <b>üìà Score R¬≤ (coefficient de d√©termination) :</b><br>
            <span style="font-size:22px;color:#1e90ff;"><b>{score:.3f}</b></span>
            </div>
            """, unsafe_allow_html=True)

        st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("---")
        st.markdown(
            "<div style='background-color:#fff9e6;padding:10px;border-radius:8px;border-left:5px solid #ffd24d;'>"
            "<h4>üß© Analyse m√©tier</h4>",
            unsafe_allow_html=True
        )

        # Analyse des variables
        if model_type == "R√©gression Lin√©aire":
            st.subheader("üìä Impact marginal estim√© des variables")

            coef_values = model.coef_
            baseline = model.predict(X).mean()

            marginal_effects = []

            for var, coef in zip(X.columns, coef_values):
                if X[var].nunique() <= 10:
                    # On suppose binaire 0/1 ‚Üí impact direct = coef
                    impact = coef
                else:
                    # Impact d‚Äôune variation d‚Äôun √©cart-type
                    std = X[var].std()
                    impact = coef * std

                marginal_effects.append((var, impact))

            # Construction DataFrame
            marginal_df = pd.DataFrame(marginal_effects, columns=["Variable", "Impact marginal (‚Ç¨)"])
            marginal_df["Impact_num"] = 100 * marginal_df["Impact marginal (‚Ç¨)"] / baseline

            # Tri et formatage
            marginal_df = marginal_df.sort_values(by="Impact_num", key=abs, ascending=False)
            marginal_df["Impact (%)"] = marginal_df["Impact_num"].map(lambda x: f"+{x:.2f}%" if x > 0 else f"{x:.2f}%")
            marginal_df["Impact marginal (‚Ç¨)"] = marginal_df["Impact marginal (‚Ç¨)"].map(lambda x: f"+{x:,.0f} ‚Ç¨" if x > 0 else f"{x:,.0f} ‚Ç¨")
            marginal_df = marginal_df.drop(columns=["Impact_num"])

            st.dataframe(marginal_df)

            st.subheader("üìå R√©partition des collaborateurs")

            st.markdown("""
            <small>‚ÑπÔ∏è Ces graphiques montrent la **r√©partition de la r√©mun√©ration** (variable cible) en fonction de chaque variable explicative s√©lectionn√©e.
            Ils permettent de **visualiser les relations brutes**, ind√©pendamment du mod√®le.</small>
            """, unsafe_allow_html=True)

            for var in features:
                if var not in df.columns:
                    continue  # Variable supprim√©e / transform√©e

                try:
                    fig, ax = plt.subplots()
                    if df[var].nunique() <= 10:
                        # Variable cat√©gorielle : boxplot
                        df_box = pd.concat([df[var].astype(str), df[target]], axis=1).dropna()
                        df_box.boxplot(column=target, by=var, ax=ax)
                        ax.set_title(f"{target} selon {var}")
                        ax.set_ylabel(target)
                        ax.set_xlabel(var)
                        plt.suptitle("")
                    else:
                        # Variable continue : scatterplot + droite de tendance
                        x = pd.to_numeric(df[var], errors='coerce')
                        y = pd.to_numeric(df[target], errors='coerce')
                        mask = x.notna() & y.notna()
                        x = x[mask]
                        y = y[mask]

                        ax.scatter(x, y, alpha=0.3)

                        if x.nunique() > 1:
                            coeffs = np.polyfit(x, y, deg=1)
                            poly_eq = np.poly1d(coeffs)
                            x_vals = np.linspace(x.min(), x.max(), 100)
                            y_vals = poly_eq(x_vals)
                            ax.plot(x_vals, y_vals, color='red', linestyle='--', label='Tendance lin√©aire')
                            ax.legend()

                        ax.set_xlabel(var)
                        ax.set_ylabel(target)
                        ax.set_title(f"{target} en fonction de {var}")

                    st.pyplot(fig)

                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Erreur lors de la visualisation pour la variable {var} : {e}")



        else:
            st.subheader("üåü Importance des variables")
            st.markdown("""
            <small>‚ÑπÔ∏è Cette mesure refl√®te l'**influence moyenne** de chaque variable dans les d√©cisions du mod√®le.
            Plus une variable a une importance √©lev√©e, plus elle contribue aux pr√©dictions.</small>
            """, unsafe_allow_html=True)
            # Recalibrage avec permutation + SHAP-like (impact marginal)
            st.subheader("üìä Impact marginal estim√© des variables")

            # Pr√©diction moyenne de r√©f√©rence
            baseline = model.predict(X).mean()

            marginal_effects = []

            for var in X.columns:
                if X[var].nunique() <= 10:
                    # On teste la variation "0 ‚Üí 1"
                    X_alt = X.copy()
                    if 0 in X[var].values and 1 in X[var].values:
                        X_alt[var] = 1
                        y_alt = model.predict(X_alt)
                        impact = y_alt.mean() - baseline
                        marginal_effects.append((var, impact))
                else:
                    # Pour variables continues : +1 std
                    X_alt = X.copy()
                    X_alt[var] += X[var].std()
                    y_alt = model.predict(X_alt)
                    impact = y_alt.mean() - baseline
                    marginal_effects.append((var, impact))

            # Cr√©ation du DataFrame brut
            marginal_df = pd.DataFrame(marginal_effects, columns=["Variable", "Impact marginal (‚Ç¨)"])
            marginal_df["Impact_num"] = 100 * marginal_df["Impact marginal (‚Ç¨)"] / baseline  # colonne num√©rique temporaire

            # Tri sur les valeurs num√©riques
            marginal_df = marginal_df.sort_values(by="Impact_num", key=abs, ascending=False)

            # Formatage visuel apr√®s le tri
            marginal_df["Impact (%)"] = marginal_df["Impact_num"].map(lambda x: f"+{x:.2f}%" if x > 0 else f"{x:.2f}%")
            marginal_df["Impact marginal (‚Ç¨)"] = marginal_df["Impact marginal (‚Ç¨)"].map(lambda x: f"+{x:,.0f} ‚Ç¨" if x > 0 else f"{x:,.0f} ‚Ç¨")

            # Suppression de la colonne temporaire
            marginal_df = marginal_df.drop(columns=["Impact_num"])

            # Affichage
            st.dataframe(marginal_df)



            # ‚úÖ Partial Dependence Plot pour Random Forest
            if model_type == "Random Forest":
                from sklearn.inspection import PartialDependenceDisplay

                st.subheader("üìà Analyse partielle de d√©pendance (PDP)")
                selected_pdp_var = st.selectbox("üîé Choisissez une variable pour visualiser son effet marginal", X.columns.tolist())

                fig, ax = plt.subplots(figsize=(8, 5))
                PartialDependenceDisplay.from_estimator(model, X, [selected_pdp_var], ax=ax)
                st.pyplot(fig)

                st.markdown("""
                <small>‚ÑπÔ∏è Le **PDP** (Partial Dependence Plot) montre **l'effet marginal** d'une variable explicative sur la pr√©diction :
                autrement dit, comment la variable influence en moyenne la r√©mun√©ration estim√©e,
                en maintenant les autres variables constantes.</small>
                """, unsafe_allow_html=True)

            if model_type == "Arbre de D√©cision":
                st.subheader("üå≥ Arbre de D√©cision")
                st.markdown("""
                <small>‚ÑπÔ∏è Cet arbre repr√©sente les **r√®gles de segmentation** utilis√©es par le mod√®le.
                Chaque n≈ìud indique une **condition** sur une variable explicative, conduisant √† une estimation du salaire.
                </small>
                """, unsafe_allow_html=True)
                fig, ax = plt.subplots(figsize=(20, 10))
                plot_tree(model, feature_names=X.columns, filled=True, rounded=True, fontsize=10, max_depth=4, ax=ax)
                st.pyplot(fig)

            st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("---")
        st.markdown(
            "<div style='background-color:#ffe6e6;padding:10px;border-radius:8px;border-left:5px solid #ff4d4d;'>"
            "<h4>üõ†Ô∏è Contr√¥le qualit√© du mod√®le</h4>",
            unsafe_allow_html=True
        )


        st.subheader("üìâ R√©sultats pr√©dits vs r√©els")

        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))

        st.markdown(f"""
        <div style="background-color:#f9f9f9;padding:10px;border-radius:8px;border:1px solid #ccc;">
        <b>üìå R√©sum√© des performances :</b><br>
        - <b>MAE (erreur absolue moyenne)</b> : {mae:.2f}  
        - <b>RMSE (racine de l'erreur quadratique moyenne)</b> : {rmse:.2f}  
        - <b>R¬≤</b> : {score:.3f}
        </div>
        """, unsafe_allow_html=True)

        fig, ax = plt.subplots()
        ax.scatter(y_test, y_pred, alpha=0.5)
        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='y = x')
        ax.set_xlabel("Valeurs r√©elles")
        ax.set_ylabel("Valeurs pr√©dites")
        ax.set_title("Pr√©diction vs Valeurs r√©elles")
        ax.legend()
        st.pyplot(fig)

        # Histogramme des r√©sidus
        st.subheader("üìä Distribution des r√©sidus")
        st.markdown("""
        <small>‚ÑπÔ∏è Les **r√©sidus** correspondent √† l‚Äôerreur de pr√©diction : `valeur r√©elle - valeur pr√©dite`.
        Un bon mod√®le a des r√©sidus centr√©s autour de 0. L‚Äôhistogramme permet de visualiser la r√©partition de ces erreurs.
        </small>
        """, unsafe_allow_html=True)
        residuals = y_test - y_pred
        fig2, ax2 = plt.subplots()
        ax2.hist(residuals, bins=30, color='gray', edgecolor='black')
        ax2.set_title("Histogramme des r√©sidus")
        ax2.set_xlabel("Erreur (y - y_pred)")
        ax2.set_ylabel("Fr√©quence")
        st.pyplot(fig2)

        st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("---")
        st.markdown(
            "<div style='background-color:#e6ffee;padding:10px;border-radius:8px;border-left:5px solid #33cc66;'>"
            "<h4>üîç Outil de pr√©diction personnalis√©e</h4>",
            unsafe_allow_html=True
        )

        # üîç Estimation personnalis√©e
        st.subheader("üßÆ Estimation personnalis√©e de la r√©mun√©ration")

        st.markdown("""
        <small>Compl√©tez les caract√©ristiques du profil ci-dessous pour obtenir une estimation de la r√©mun√©ration
        selon le mod√®le s√©lectionn√© et les variables explicatives retenues.</small>
        """, unsafe_allow_html=True)

        user_input = {}
        for var in features:
            if df[var].dtype == 'object' or df[var].nunique() <= 10:
                # Afficher uniquement les valeurs d√©j√† pr√©sentes dans la base utilis√©e
                unique_values = df[var].dropna().unique()
                # Si m√©lange de types non comparables, on affiche tel quel sans tri
                try:
                    options = sorted(unique_values.tolist())
                except TypeError:
                    options = list(unique_values)

                user_input[var] = st.selectbox(f"{var}", options, key=f"user_input_{var}")

            else:
                val_min = float(df[var].min())
                val_max = float(df[var].max())
                default = float(df[var].median())
                user_input[var] = st.slider(f"{var}", int(val_min), int(val_max), int(default), step=1, key=f"user_input_{var}")

        # Transformation identique au training
        input_df = pd.DataFrame([user_input])
        input_df = pd.get_dummies(input_df)
        # Forcer √† ne garder que les colonnes connues du mod√®le
        missing_cols = [col for col in X.columns if col not in input_df.columns]
        for col in missing_cols:
            input_df[col] = 0
        input_df = input_df[X.columns]
        input_df = input_df.reindex(columns=X.columns, fill_value=0)
        st.write("‚úÖ Variables utilis√©es pour la pr√©diction :", input_df)

        try:
            if model_type == "Arbre de D√©cision":
                leaf_id = model.apply(input_df)[0]
                prediction = model.tree_.value[leaf_id][0][0]
            else:
                prediction = model.predict(input_df)[0]

            st.success(f"üí∞ Estimation de la variable ¬´ {target} ¬ª pour ce profil : **{prediction:.2f}**")
        except Exception as e:
            st.error("‚ùå Impossible de calculer la r√©mun√©ration avec les donn√©es saisies.")
    
        st.markdown("</div>", unsafe_allow_html=True)
